import streamlit as st
import pandas as pd
import altair as alt
import fitz  # PyMuPDF
from dotenv import load_dotenv
load_dotenv()

from config import MODELS, USD_TO_JPY, get_api_key, ModelConfig
from providers import LLMResponse, OpenAIClient, AnthropicClient, GoogleClient, XAIClient

st.set_page_config(page_title="LLMæ€§èƒ½æ¯”è¼ƒ", page_icon="ğŸ¤–", layout="wide")

CLIENTS = {"openai": OpenAIClient, "anthropic": AnthropicClient, "google": GoogleClient, "xai": XAIClient}

def get_model_params() -> dict:
    """ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—"""
    return {
        # GPT-5.1
        "gpt51_reasoning": st.session_state.get("gpt51_reasoning", "medium"),
        "gpt51_verbosity": st.session_state.get("gpt51_verbosity", "medium"),
        "gpt51_max_tokens": st.session_state.get("gpt51_max_tokens", 10000),
        # GPT-5 / mini / nano
        "gpt5_reasoning": st.session_state.get("gpt5_reasoning", "medium"),
        "gpt5_verbosity": st.session_state.get("gpt5_verbosity", "medium"),
        "gpt5_max_tokens": st.session_state.get("gpt5_max_tokens", 10000),
        # Claude
        "claude_thinking": st.session_state.get("claude_thinking", False),
        "claude_budget": st.session_state.get("claude_budget", 8000),
        "claude_temp": st.session_state.get("claude_temp", 0.0),
        "claude_max_tokens": st.session_state.get("claude_max_tokens", 10000),
        # Gemini 3 Pro
        "gemini3_thinking_level": st.session_state.get("gemini3_thinking_level", "low"),
        "gemini3_max_tokens": st.session_state.get("gemini3_max_tokens", 10000),
        # Gemini 2.5ç³»ï¼ˆå…±é€šï¼‰
        "gemini_temp": st.session_state.get("gemini_temp", 0.0),
        "gemini_max_tokens": st.session_state.get("gemini_max_tokens", 10000),
        # Grokï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«å…±é€šï¼‰
        "grok_temp": st.session_state.get("grok_temp", 0.0),
        "grok_max_tokens": st.session_state.get("grok_max_tokens", 10000),
    }

def run_generation(model: ModelConfig, prompt: str, params: dict) -> LLMResponse:
    api_key = get_api_key(model.provider)
    if not api_key:
        return LLMResponse("", 0, 0, 0, model.id, f"APIã‚­ãƒ¼æœªè¨­å®š: {model.provider.upper()}_API_KEY", 0)

    client = CLIENTS[model.provider](api_key)

    if model.provider == "openai":
        if "gpt-5.1" in model.id:
            return client.generate(prompt, model.id,
                reasoning_effort=params["gpt51_reasoning"],
                verbosity=params["gpt51_verbosity"],
                max_completion_tokens=params["gpt51_max_tokens"])
        else:
            return client.generate(prompt, model.id,
                reasoning_effort=params["gpt5_reasoning"],
                verbosity=params["gpt5_verbosity"],
                max_completion_tokens=params["gpt5_max_tokens"])
    elif model.provider == "anthropic":
        return client.generate(prompt, model.id,
            extended_thinking=params["claude_thinking"],
            budget_tokens=params["claude_budget"],
            temperature=params["claude_temp"],
            max_tokens=params["claude_max_tokens"])
    elif model.provider == "google":
        if "gemini-3" in model.id:
            return client.generate(prompt, model.id,
                thinking_level=params["gemini3_thinking_level"],
                max_tokens=params["gemini3_max_tokens"])
        else:
            return client.generate(prompt, model.id,
                temperature=params["gemini_temp"],
                max_tokens=params["gemini_max_tokens"])
    else:  # xaiï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«å…±é€šï¼‰
        return client.generate(prompt, model.id,
            temperature=params["grok_temp"],
            max_tokens=params["grok_max_tokens"])

def extract_pdf_text(file) -> str:
    """PDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    doc = fitz.open(stream=file.read(), filetype="pdf")
    text = ""
    for page in doc:
        text += page.get_text()
    doc.close()
    return text

def get_prompt_input(key: str) -> str:
    prompt = st.text_area("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ", height=200, key=f"{key}_prompt", placeholder="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›...\n\n{file_content} ã§ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’æŒ¿å…¥å¯èƒ½")
    with st.expander("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ·»ä»˜"):
        file = st.file_uploader("ãƒ•ã‚¡ã‚¤ãƒ«", type=["txt", "md", "csv", "json", "py", "pdf"], key=f"{key}_file")
        if file:
            try:
                if file.name.endswith(".pdf"):
                    content = extract_pdf_text(file)
                    st.success(f"âœ… PDFèª­ã¿è¾¼ã¿å®Œäº†: {file.name} ({len(content):,} æ–‡å­—)")
                else:
                    content = file.read().decode("utf-8")
                    st.success(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {file.name} ({len(content):,} æ–‡å­—)")

                st.code(content[:1000])
                if len(content) > 1000:
                    st.caption(f"... ä»¥ä¸‹çœç•¥ï¼ˆæ®‹ã‚Š {len(content) - 1000:,} æ–‡å­—ï¼‰")

                if "{file_content}" in prompt:
                    prompt = prompt.replace("{file_content}", content)
                else:
                    prompt = f"{prompt}\n\n{content}" if prompt.strip() else content
            except Exception as e:
                st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    return prompt

def render_sidebar():
    """ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šUIã‚’è¡¨ç¤º"""
    with st.sidebar:
        st.header("âš™ï¸ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š")

        # GPT-5.1
        st.subheader("GPT-5.1")
        st.selectbox("reasoning_effort", options=["none", "low", "medium", "high"], index=2, key="gpt51_reasoning",
            help="æ¨è«–ã®æ·±ã•ã€‚noneã§æ¨è«–ãªã—")
        st.selectbox("verbosity", options=["low", "medium", "high"], index=1, key="gpt51_verbosity",
            help="å‡ºåŠ›ã®è©³ç´°åº¦")
        st.slider("max_completion_tokens", 1000, 16000, 10000, 1000, key="gpt51_max_tokens")

        st.divider()

        # GPT-5 / mini / nano
        st.subheader("GPT-5 / mini / nano")
        st.selectbox("reasoning_effort", options=["low", "medium", "high"], index=1, key="gpt5_reasoning",
            help="æ¨è«–ã®æ·±ã•")
        st.selectbox("verbosity", options=["low", "medium", "high"], index=1, key="gpt5_verbosity",
            help="å‡ºåŠ›ã®è©³ç´°åº¦")
        st.slider("max_completion_tokens", 1000, 16000, 10000, 1000, key="gpt5_max_tokens")

        st.divider()

        # Claude
        st.subheader("Claude Sonnet / Haiku")
        thinking_on = st.toggle("extended_thinking", key="claude_thinking",
            help="æ‹¡å¼µæ€è€ƒãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–")
        if thinking_on:
            st.slider("budget_tokens", 1024, 32000, 8000, 1024, key="claude_budget",
                help="æ€è€ƒãƒˆãƒ¼ã‚¯ãƒ³ã®äºˆç®—")
        else:
            st.slider("temperature", 0.0, 1.0, 0.0, 0.1, key="claude_temp")
        st.slider("max_tokens", 1000, 16000, 10000, 1000, key="claude_max_tokens")

        st.divider()

        # Gemini 3 Pro
        st.subheader("Gemini 3 Pro")
        st.selectbox("thinking_level", options=["low", "high"], index=0, key="gemini3_thinking_level",
            help="æ¨è«–ã®æ·±ã•ã€‚lowã§é«˜é€Ÿã€highã§ç²¾åº¦é‡è¦–")
        st.slider("max_tokens", 1000, 16000, 10000, 1000, key="gemini3_max_tokens")

        st.divider()

        # Gemini 2.5ç³»
        st.subheader("Gemini 2.5ç³»")
        st.slider("temperature", 0.0, 1.0, 0.0, 0.1, key="gemini_temp")
        st.slider("max_tokens", 1000, 16000, 10000, 1000, key="gemini_max_tokens")

        st.divider()

        # Grokï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«å…±é€šï¼‰
        st.subheader("Grok")
        st.slider("temperature", 0.0, 1.0, 0.0, 0.1, key="grok_temp")
        st.slider("max_tokens", 1000, 16000, 10000, 1000, key="grok_max_tokens")

def main():
    render_sidebar()
    params = get_model_params()

    st.title("LLMæ€§èƒ½æ¯”è¼ƒ")

    tab1, tab2 = st.tabs(["å˜ä½“ãƒ†ã‚¹ãƒˆ", "æ¯”è¼ƒãƒ†ã‚¹ãƒˆ"])
    all_models = [m for ms in MODELS.values() for m in ms]

    with tab1:
        options = {m.name: m for m in all_models}
        model = options[st.selectbox("ãƒ¢ãƒ‡ãƒ«", list(options.keys()))]
        prompt = get_prompt_input("single")

        if st.button("å®Ÿè¡Œ", type="primary", key="run1"):
            if prompt.strip():
                with st.spinner(f"{model.name} ç”Ÿæˆä¸­..."):
                    r = run_generation(model, prompt, params)
                if r.error:
                    st.error(r.error)
                else:
                    st.metric("æ™‚é–“", f"{r.latency_ms/1000:.2f}ç§’")
                    st.metric("ã‚³ã‚¹ãƒˆ", f"Â¥{r.calculate_cost(model.input_price, model.output_price) * USD_TO_JPY:.4f}")
                    st.text(r.content)

    with tab2:
        cols = st.columns(4)
        selected = []
        for i, (p, ms) in enumerate(MODELS.items()):
            with cols[i]:
                for m in ms:
                    if st.checkbox(m.name, value=False, key=f"cmp_{m.id}"):
                        selected.append(m)

        prompt = get_prompt_input("compare")

        if st.button("æ¯”è¼ƒå®Ÿè¡Œ", type="primary", key="run2"):
            if selected and prompt.strip():
                results = {}
                progress_bar = st.progress(0, text="æº–å‚™ä¸­...")

                for i, m in enumerate(selected):
                    progress_bar.progress(i / len(selected), text=f"{m.name} ç”Ÿæˆä¸­... ({i+1}/{len(selected)})")
                    results[m.id] = run_generation(m, prompt, params)

                progress_bar.progress(1.0, text="å®Œäº†")

                # ã‚°ãƒ©ãƒ•ãƒ»è¡¨ç”¨ãƒ‡ãƒ¼ã‚¿
                chart_data = []
                for m in selected:
                    r = results[m.id]
                    if not r.error:
                        row = {
                            "ãƒ¢ãƒ‡ãƒ«": m.name,
                            "æ™‚é–“(ç§’)": r.latency_ms / 1000,
                            "å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³": r.input_tokens,
                            "å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³": r.output_tokens,
                            "ã‚³ã‚¹ãƒˆ(Â¥)": r.calculate_cost(m.input_price, m.output_price) * USD_TO_JPY,
                        }
                        if r.reasoning_tokens > 0:
                            row["æ€è€ƒãƒˆãƒ¼ã‚¯ãƒ³"] = r.reasoning_tokens
                        chart_data.append(row)
                    else:
                        st.error(f"{m.name}: {r.error}")

                if chart_data:
                    df = pd.DataFrame(chart_data)

                    # ã‚°ãƒ©ãƒ•è¡¨ç¤º
                    col1, col2 = st.columns(2)
                    with col1:
                        st.subheader("â±ï¸ ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“")
                        df["æ™‚é–“ãƒ©ãƒ™ãƒ«"] = df["æ™‚é–“(ç§’)"].apply(lambda x: f"{x:.2f}ç§’")
                        bars = alt.Chart(df).mark_bar().encode(
                            x=alt.X("ãƒ¢ãƒ‡ãƒ«:N", sort=None, title=None),
                            y=alt.Y("æ™‚é–“(ç§’):Q", title="ç§’"),
                            color=alt.Color("ãƒ¢ãƒ‡ãƒ«:N", legend=None),
                        )
                        text = bars.mark_text(dy=-10, fontSize=14).encode(text="æ™‚é–“ãƒ©ãƒ™ãƒ«:N")
                        st.altair_chart(bars + text, use_container_width=True)

                    with col2:
                        st.subheader("ğŸ’° ã‚³ã‚¹ãƒˆ")
                        df["ã‚³ã‚¹ãƒˆãƒ©ãƒ™ãƒ«"] = df["ã‚³ã‚¹ãƒˆ(Â¥)"].apply(lambda x: f"Â¥{x:.4f}")
                        bars = alt.Chart(df).mark_bar().encode(
                            x=alt.X("ãƒ¢ãƒ‡ãƒ«:N", sort=None, title=None),
                            y=alt.Y("ã‚³ã‚¹ãƒˆ(Â¥):Q", title="å††"),
                            color=alt.Color("ãƒ¢ãƒ‡ãƒ«:N", legend=None),
                        )
                        text = bars.mark_text(dy=-10, fontSize=14).encode(text="ã‚³ã‚¹ãƒˆãƒ©ãƒ™ãƒ«:N")
                        st.altair_chart(bars + text, use_container_width=True)

                    # è¡¨
                    st.dataframe(df.drop(columns=["æ™‚é–“ãƒ©ãƒ™ãƒ«", "ã‚³ã‚¹ãƒˆãƒ©ãƒ™ãƒ«"]).style.format({"æ™‚é–“(ç§’)": "{:.2f}", "ã‚³ã‚¹ãƒˆ(Â¥)": "Â¥{:.4f}"}), use_container_width=True)

                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹è¡¨ç¤º
                st.subheader("ğŸ“ ãƒ¬ã‚¹ãƒãƒ³ã‚¹")
                for m in selected:
                    r = results[m.id]
                    with st.expander(m.name, expanded=True):
                        if r.error:
                            st.error(r.error)
                        else:
                            st.text(r.content)

if __name__ == "__main__":
    main()
